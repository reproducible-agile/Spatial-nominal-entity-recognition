{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Spatial Nominal Entity Recognition models\n",
    "\n",
    "This notebook presents the evaluation of the models trained for Spatial Nominal Entity Recognition and proposed in \n",
    "\n",
    "> Amine Medad, Mauro Gaio Ludovic Moncla, Sébastien Mustière, and Yannick Le Nir. Comparing supervised learning algorithms for Spatial Nominal Entity recognition. The 23rd AGILE International Conference on Geographic Information Science. 2020\n",
    "\n",
    "This paper presents a methodology comparing five supervised machine learning algorithms for the automatic identification of SNoE from raw texts. The approach uses a pre-trained WEs model as input according to the TL principle. The WEs used as input data for these algorithms, come from the FastText model pre-trained on a huge corpus of generic texts in French. The FastText model was chosen because it produced better results, compared to other equivalent WEs models, on so-called morphological rich languages such as French. \n",
    "\n",
    "The experimental results demonstrate: 1) the feasibility of our approach for the SNoE recognition task, 2) the importance of the context on this kind of task. Thanks to the use of the principle of transfer learning we have been able to show that it is possible to test methodological and algorithmic choices by relying on small corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/treetaggerwrapper.py:740: FutureWarning: Possible nested set at position 8\n",
      "  re.IGNORECASE | re.VERBOSE)\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/treetaggerwrapper.py:2044: FutureWarning: Possible nested set at position 152\n",
      "  re.VERBOSE | re.IGNORECASE)\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import treetaggerwrapper\n",
    "from keras.models import load_model\n",
    "from gensim.models import fasttext\n",
    "from joblib import load\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_ngrams(sentences, ngram_size, fr_nouns_file):\n",
    "\n",
    "    ngrams = []\n",
    "    context_size = int(ngram_size / 2)\n",
    "    tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr', TAGINENC='utf-8', TAGOUTENC='utf-8')\n",
    "\n",
    "    with open(fr_nouns_file, \"r\") as file:\n",
    "        fr_nouns = file.readlines()\n",
    "\n",
    "    for s in sentences:\n",
    "        s = s.replace(';', '')\n",
    "        s = s.replace(\"'\", chr(39))\n",
    "        s = s.replace('\\'', chr(39))\n",
    "        s = s.replace(\"d\\'\", \" deeee \")\n",
    "        s = s.replace(\"l\\'\", \" leeee \")\n",
    "\n",
    "        sentence_tagged = treetaggerwrapper.make_tags(tagger.tag_text(s))\n",
    "\n",
    "        try:\n",
    "            sentence = list(np.array(sentence_tagged)[:, 0])  # getting only the token (not lemmas and POS)\n",
    "        except IndexError:\n",
    "            pass\n",
    "            \n",
    "        for i, token in enumerate(sentence):\n",
    "            if token == \"leeee\":\n",
    "                sentence[i] = \"l\\'\"\n",
    "            if token == 'deeee':\n",
    "                sentence[i] = \"d\\'\"\n",
    "\n",
    "        index_left = sentence.index('[')\n",
    "        index_right = sentence.index(']')\n",
    "\n",
    "        phrase_ngram = []\n",
    "\n",
    "        # add left context\n",
    "        for i in range(context_size):\n",
    "            try:\n",
    "                phrase_ngram.append(sentence[index_left - context_size + i])\n",
    "            except IndexError:\n",
    "                # when there is not enough words (ex: pivot word starting the sentence)\n",
    "                phrase_ngram.append(random.choice(fr_nouns).rstrip())\n",
    "\n",
    "        # add pivot token(s) (can contain several tokens)\n",
    "        phrase_ngram.append(' '.join(sentence[index_left + 1:index_right]))\n",
    "\n",
    "        # add right context\n",
    "        for i in range(context_size):\n",
    "            try:\n",
    "                phrase_ngram.append(sentence[index_right + 1 + i])\n",
    "            except IndexError:\n",
    "                # when there is not enough words (ex: pivot word starting the sentence)\n",
    "                phrase_ngram.append(random.choice(fr_nouns).rstrip())\n",
    "\n",
    "        ngrams.append(phrase_ngram)\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(ngram_size, input_data, we_vector_size, fasttext_wv):\n",
    "\n",
    "    data_vec = np.array([])\n",
    "\n",
    "    for phrase in input_data:\n",
    "        phrase_vec = np.array([])\n",
    "\n",
    "        for word in phrase:\n",
    "            word = word.replace(\"’\", \"\\'\")\n",
    "            vec = fasttext_wv[word]\n",
    "            phrase_vec = np.append(phrase_vec, vec)\n",
    "\n",
    "        data_vec = np.append(data_vec, phrase_vec)\n",
    "\n",
    "    data_vec = np.reshape(data_vec, (len(input_data), ngram_size, we_vector_size))\n",
    "\n",
    "    return data_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = './data/corpus_validation.csv'\n",
    "train_corpus_filepath = './data/corpus_train.csv'\n",
    "we_vector_size = 300\n",
    "fr_nouns_file = './data/French_nouns.txt'\n",
    "model_fasttext = './data/cc.fr.300.bin'\n",
    "\n",
    "keras_models = ['GRU', 'MLP_PCA', 'MLP_AE']\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Load input data... \n",
      "\n",
      "   idf  labels                                          sentences  \\\n",
      "0  166       1  la balade peut se poursuivre autour du lac ou ...   \n",
      "1  303       1  le sentier grimpe au-dessus du hameau avec un ...   \n",
      "2  199       1  ( 9 ) poursuivre la descente vers la droite en...   \n",
      "3  394       1  continuer un petit peu sur l'arête puis descen...   \n",
      "4  313       1  (3) À la [patte d'oie], laisser le départ à dr...   \n",
      "\n",
      "         pivot_words                src               alea  \n",
      "0               cols  corpus_validation  0,085618299510875  \n",
      "1            passage  corpus_validation  0,295723408093251  \n",
      "2             église  corpus_validation  0,942577511847241  \n",
      "3  sentier de montée  corpus_validation  0,543919977290256  \n",
      "4        patte d'oie  corpus_validation    0,8779708338724  \n"
     ]
    }
   ],
   "source": [
    "print('** Load input data... \\n')\n",
    "df = pd.read_csv(input_data, delimiter=';', names=['idf', 'labels', 'sentences', 'pivot_words', 'src', 'alea'])\n",
    "\n",
    "print(df.head(5))\n",
    "\n",
    "y_test = df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Loading fastText model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"** Loading fastText model...\\n\")\n",
    "fasttext_model = fasttext.load_facebook_vectors(model_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(sentences, ngram_size, fr_nouns_file, fasttext_model, we_vector_size):\n",
    "    print('** Transform sentences to ' + str(ngram_size) + ' ngrams... \\n')\n",
    "    ngrams_list = sentences_to_ngrams(sentences, ngram_size, fr_nouns_file)\n",
    "    #print(ngrams_list)\n",
    "\n",
    "    print('** Vectorisation of inputs... \\n')\n",
    "    x_test = vectorization(ngram_size, ngrams_list, we_vector_size, fasttext_model)\n",
    "    \n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmodel(model_path, algorithm, keras_models):\n",
    "    print('** Loading model ' + model_path + ' \\n')\n",
    "\n",
    "    if algorithm in keras_models:\n",
    "        clf = load_model(model_path)\n",
    "    else:\n",
    "        clf = load(model_path)\n",
    "        \n",
    "    return clf\n",
    "\n",
    "\n",
    "def prediction(clf, x_test, y_test, ngram_size, we_vector_size, algorithm):\n",
    "    print('** Predicting... \\n')\n",
    "\n",
    "    if algorithm == 'RF' or algorithm == 'SVM' or algorithm == 'MLP_AE' or algorithm == 'MLP_PCA':\n",
    "        x_test = np.reshape(x_test, (len(x_test), ngram_size * we_vector_size))\n",
    "\n",
    "    if algorithm == 'MLP_PCA':\n",
    "        df_train = pd.read_csv(train_corpus_filepath, delimiter=';', names=['idf', 'labels', 'sentences', 'pivot_words', 'alea'])\n",
    "        \n",
    "        x_train = preprocess_input(df_train['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "        x_train = np.reshape(x_train, (len(x_train), ngram_size * we_vector_size))\n",
    "\n",
    "        #pca = PCA(0.99)\n",
    "        if ngram_size == 1:\n",
    "            pca = PCA(n_components = 87, random_state=1)\n",
    "        if ngram_size == 5:\n",
    "            pca = PCA(n_components = 295, random_state=1)\n",
    "        if ngram_size == 7:\n",
    "            pca = PCA(n_components = 369, random_state=1)\n",
    "        \n",
    "        pca.fit(x_train)\n",
    "        \n",
    "        x_test = pca.transform(x_test)\n",
    "\n",
    "    if algorithm in keras_models:\n",
    "        #y_pred = clf.predict_classes(x_test)\n",
    "        score = clf.evaluate(x_test, y_test)\n",
    "        accuracy = score[1]\n",
    "\n",
    "    if algorithm == 'RF' or algorithm == 'SVM':\n",
    "        #y_pred = clf.predict(x_test)\n",
    "        accuracy = clf.score(x_test, y_test)\n",
    "        \n",
    "    #precision = precision_score(y_test, y_pred)\n",
    "    #recall = recall_score(y_test, y_pred)\n",
    "    #f1 = f1_score(y_test, y_pred)\n",
    "    #accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "\n",
    "For GRU models, hyper-parameters include the number of GRU units (5,10,100,1000,100), GRU units activation function (hyperbolic tangent), recurrent activation function (hyperbolic tangent), dropout (0.0, 0.3, 0.5, 0.8, 0.9), recurrent dropout (0.0, 0.3, 0.5, 0.8, 0.9), dense activation function (hyperbolic tangent, sigmoid), the number of epochs for training (500, 1000, 2000), the optimiser (adam) with learning rate (0.001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 1 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/GRU_1gram.h5 \n",
      "\n",
      "** Predicting... \n",
      "\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.3501 - accuracy: 0.6701\n",
      "accuracy:  0.6701030731201172\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/GRU_1gram.h5'\n",
    "algorithm = 'GRU'\n",
    "ngram_size = 1\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 5 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/GRU_5grams.h5 \n",
      "\n",
      "** Predicting... \n",
      "\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.7235 - accuracy: 0.7629\n",
      "accuracy:  0.7628865838050842\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/GRU_5grams.h5'\n",
    "algorithm = 'GRU'\n",
    "ngram_size = 5\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 7 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/GRU_7grams.h5 \n",
      "\n",
      "** Predicting... \n",
      "\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.7938\n",
      "accuracy:  0.7938144207000732\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/GRU_7grams.h5'\n",
    "algorithm = 'GRU'\n",
    "ngram_size = 7\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP + PCA\n",
    "\n",
    "For MLP+PCA hyper-parameters include activation function for each layer (Exponential Linear Unit, Rectified Linear Unit, Softplus), the output layer activation function (sigmoid), dropout (0.0, 0.3, 0.5), PCA information, the optimiser (adam) with learning rate (0.001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 1 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/MLP_PCA_1gram.h5 \n",
      "\n",
      "** Predicting... \n",
      "\n",
      "** Transform sentences to 1 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 2.4451 - accuracy: 0.4897\n",
      "accuracy:  0.48969072103500366\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/MLP_PCA_1gram.h5'\n",
    "algorithm = 'MLP_PCA'\n",
    "ngram_size = 1\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 5 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/MLP_PCA_5grams.h5 \n",
      "\n",
      "** Predicting... \n",
      "\n",
      "** Transform sentences to 5 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 15.0976 - accuracy: 0.6546\n",
      "accuracy:  0.6546391844749451\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/MLP_PCA_5grams.h5'\n",
    "#model_path = 'MLP_ACP_models/MLP_PCA_relu_relu_sigmoid_32_6_adam_dynamique_0.5_5.h5'\n",
    "algorithm = 'MLP_PCA'\n",
    "ngram_size = 5\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 7 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/MLP_PCA_7grams.h5 \n",
      "\n",
      "** Predicting... \n",
      "\n",
      "** Transform sentences to 7 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 13.9287 - accuracy: 0.5928\n",
      "accuracy:  0.592783510684967\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/MLP_PCA_7grams.h5'\n",
    "algorithm = 'MLP_PCA'\n",
    "ngram_size = 7\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP + AE\n",
    "\n",
    "For MLP+AE the hyper-parameters are the same as in the MLP+AE except for the dropout (0.0, 0.5, 0.9), and the dimension of the encoding layer (500)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 1 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/MLP_AE_1gram.h5 \n",
      "\n",
      "** Predicting... \n",
      "\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.7019 - accuracy: 0.6856\n",
      "accuracy:  0.6855670213699341\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/MLP_AE_1gram.h5'\n",
    "algorithm = 'MLP_AE'\n",
    "ngram_size = 1\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 5 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/MLP_AE_5grams.h5 \n",
      "\n",
      "** Predicting... \n",
      "\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.3300 - accuracy: 0.7474\n",
      "accuracy:  0.7474226951599121\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/MLP_AE_5grams.h5'\n",
    "algorithm = 'MLP_AE'\n",
    "ngram_size = 5\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 7 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/MLP_AE_7grams.h5 \n",
      "\n",
      "** Predicting... \n",
      "\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3850 - accuracy: 0.7680\n",
      "accuracy:  0.7680412530899048\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/MLP_AE_7grams.h5'\n",
    "algorithm = 'MLP_AE'\n",
    "ngram_size = 7\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "For the RF, hyper-parameters include the number of trees in the forest (50, 60, 70, 80, 100, 200, 300, 500), the maximum depth of the tree (1, 3, 6, 12, 15, 20, 22, 25, 27, 29, 32, 34, 36, 38, 40, 43, 46, 48, 50, 60, 65, 70, 75, 80), the function to measure the quality of a split (Gini impurity, Entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 1 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/RF_1gram.joblib \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Predicting... \n",
      "\n",
      "accuracy:  0.7061855670103093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.20.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/RF_1gram.joblib'\n",
    "algorithm = 'RF'\n",
    "ngram_size = 1\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 5 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/RF_5grams.joblib \n",
      "\n",
      "** Predicting... \n",
      "\n",
      "accuracy:  0.7268041237113402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.20.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/RF_5grams.joblib'\n",
    "algorithm = 'RF'\n",
    "ngram_size = 5\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 7 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/RF_7grams.joblib \n",
      "\n",
      "** Predicting... \n",
      "\n",
      "accuracy:  0.7474226804123711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lmoncla/.pyenv/versions/3.7.3/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/lmoncla/.pyenv/versions/3.7.3/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.20.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/RF_7grams.joblib'\n",
    "algorithm = 'RF'\n",
    "ngram_size = 7\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "For the SVM the hyper-parameters include the kernel type (Polynomial, Linear, Sigmoid, Ra- dial Basis Function), regularisation parameter (1e-3, 1e-2, 1e-1, 0.5, 1, 10, 100), the kernel coefficient gamma (1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 1 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/SVM_1gram.joblib \n",
      "\n",
      "** Predicting... \n",
      "\n",
      "accuracy:  0.6907216494845361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator SVC from version 0.20.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/SVM_1gram.joblib'\n",
    "algorithm = 'SVM'\n",
    "ngram_size = 1\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 5 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/SVM_5grams.joblib \n",
      "\n",
      "** Predicting... \n",
      "\n",
      "accuracy:  0.7525773195876289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator SVC from version 0.20.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/SVM_5grams.joblib'\n",
    "algorithm = 'SVM'\n",
    "ngram_size = 5\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Transform sentences to 7 ngrams... \n",
      "\n",
      "** Vectorisation of inputs... \n",
      "\n",
      "** Loading model ./models/SVM_7grams.joblib \n",
      "\n",
      "** Predicting... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator SVC from version 0.20.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7268041237113402\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/SVM_7grams.joblib'\n",
    "algorithm = 'SVM'\n",
    "ngram_size = 7\n",
    "\n",
    "x_test = preprocess_input(df['sentences'], ngram_size, fr_nouns_file, fasttext_model, we_vector_size)\n",
    "model = loadmodel(model_path, algorithm, keras_models)\n",
    "\n",
    "score = prediction(model, x_test, y_test, ngram_size, we_vector_size, algorithm)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
